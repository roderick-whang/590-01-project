---
title: "590-01 Final Project"
author: "Roderick Whang"
date: "4/6/2021"
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE}
rm(list = ls())
setwd("C:\\590_final")
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(gridExtra)
library(psych)
library(corrplot)
library(ggfortify)
library(factoextra)
library(class) #knn
library(gmodels) # CrossTable()
library(caret) # creatFolds()
library(caTools) #sample.split()
library(ROCR) # prediction(), performance()
library(randomForest) # Random Forest
library(caret)
library(e1071) # SVM
set.seed(2021)

df1 <- read.csv("S1.csv")
df2 <- read.csv("S2.csv")
df3 <- read.csv("S3.csv")
df4 <- read.csv("S4.csv")
df5 <- read.csv("S5.csv")
df6 <- read.csv("S6.csv")
df7 <- read.csv("S7.csv")
df8 <- read.csv("S8.csv")
df9 <- read.csv("S9.csv")
df10 <- read.csv("S10.csv")
df11 <- read.csv("S11.csv")
df12 <- read.csv("S12.csv")
df13 <- read.csv("S13.csv")
df14 <- read.csv("S14.csv")
df15 <- read.csv("S15.csv")

df <- rbind(df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15) 
df$activity <- as.factor(df$activity)
df <- subset(df, select = -c(X) )
df <- subset(df, df$activity == 1 | df$activity == 2 | df$activity == 3 | 
               df$activity ==4 | df$activity == 5| df$activity == 6 | df$activity == 7 | 
               df$activity ==8)

# df$Activity
# low = 1

# medium = 3
df$activity[df$activity== 3 | df$activity== 5 | df$activity == 6 | df$activity == 8] <- 3

# high = 2
df$activity[df$activity==2 | df$activity== 4 | df$activity == 7] <- 2


df$activity <- factor(df$activity)

a2 <- which(df$activity ==2)
a3 <- which(df$activity ==3)

a2_s <- sample(a2, 6000, replace = FALSE)
a3_s <- sample(a3, 26000, replace = FALSE)

df <- df[-c(a2_s, a3_s),]

```

```{r}
head(df)

```
```{r}
set.seed(2021) 

sample <- sample.split(df$activity, SplitRatio = .8) # dataset to split it into 80:20

df_train <- df[sample==TRUE, ]
df_test <- df[sample==FALSE, ]

X_train <- subset(df_train, select = -c(activity) ) # independent variables
y_train <- df_train[,1] # tartget variables

X_test <- subset(df_test, select = -c(activity) ) # independent variables
y_test <- df_test[,1] # tartget variables
```

## RF

```{r}
bestmtry <- tuneRF(X_train, y_train, stepFactor=1.5, improve=1e-5, ntree=700)
print(bestmtry)
```

```{r}
rf.model <- randomForest(formula = activity ~ ., data = df_train, ntree=700, mtry=3, importance = TRUE, type="prob")
rf.model
```
```{r}
varImpPlot(rf.model)
```
The Mean Decrease Accuracy plot expresses how much accuracy the model losses by excluding each variable. The more the accuracy suffers, the more important the variable is for the successful classification. The variables are presented from descending importance. The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.

```{r}
prediction_for_table <- predict(rf.model,X_test)
#table(observed=y_test,predicted=prediction_for_table)
confusionMatrix(prediction_for_table, y_test)
```
```{r}
pred_prob.rf <- predict(rf.model, X_test, decision.values = TRUE, type="prob")
colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes 
classes <- levels(df$activity)
# For each class
for (i in 1:3)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(y_test==classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(pred_prob.rf[,i],true_values)
 
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf,main="ROC Curve of RF",col=colours[i]) 
 }
 else
 {
     plot(perf,main="ROC Curve of RF",col=colours[i],add=TRUE) 
 }
 legend("bottomright",  c("class_1","class_2","class_3"), col = colours, lty= 1, horiz=TRUE)
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(paste("AUC of class_",i,":",auc.perf@y.values))
}
```


## SVM
```{r}
#set.seed(1)
#X <- sample(dim(X_train)[1], 3000, replace=FALSE)
#tune.out <- tune(svm, activity ~., data=df_train[X,], 
#                 kernel='radial', 
#                 ranges = list(cost=c(0.1,1,10,100,1000),
#                 gamma=c(0.5, 1,2,3,4)))

#summary(tune.out)
```

```{r}
svm.opt <- svm(activity ~., data=df_train, kernel='radial', type =  'C-classification',
               gamma=0.07, cost=10
               , decision.values=T, probability = TRUE)
pred <- predict(svm.opt, X_test, decision.values = TRUE, probability = TRUE)

confusionMatrix(pred, y_test)
```


```{r}
pred_prob.svm <- attr(pred, "probabilities")
colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes 
classes <- levels(df$activity)
# For each class
for (i in 1:3)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(y_test==classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(pred_prob.svm[,i],true_values)
 
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf,main="ROC Curve of SVM",col=colours[i]) 
 }
 else
 {
     plot(perf,main="ROC Curve of SVM",col=colours[i],add=TRUE) 
 }
 legend("bottomright",  c("class_1","class_2","class_3"), col = colours, lty= 1, horiz=TRUE)
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(paste("AUC of class_",i,":",auc.perf@y.values))
}
```


